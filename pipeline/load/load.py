import pandas as pd
from dotenv import load_dotenv
import sqlite3
from google.cloud import bigquery
import os

# Get project root dynamically
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__))))

# Load .env from project root
load_dotenv(os.path.join(PROJECT_ROOT, "config/.env"), override=True)



# === CONFIGURATION ===
sqlite_db_path = "/Users/danieltom/Desktop/daniel grocery project/products.db"
sqlite_table_name = "products"
bq_project = "daniel-grocery-project"
bq_dataset = "products_dataset"
bq_table = "products_2"
credentials_path = os.getenv("GOOGLE_JSON_KEY_FILE_PATH")


# === STEP 1: READ DATA FROM SQLITE ===
conn = sqlite3.connect(sqlite_db_path)
df = pd.read_sql_query(f"SELECT * FROM {sqlite_table_name}", conn)
conn.close()

print(f"Loaded {len(df)} rows from {sqlite_table_name} in SQLite.")

# === STEP 2: UPLOAD TO BIGQUERY ===
client = bigquery.Client.from_service_account_json(credentials_path, project=bq_project)
table_id = f"{bq_project}.{bq_dataset}.{bq_table}"

job = client.load_table_from_dataframe(df, table_id)

job.result()  # Waits for the job to finish

print(f"âœ… Successfully loaded {len(df)} rows into {table_id}.")

 