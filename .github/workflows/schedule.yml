name: Daily Grocery Pipeline

on:
  schedule:
    # Runs every day at 7 AM Pacific (15:00 UTC)
    - cron: "0 15 * * *"
  workflow_dispatch:

jobs:
  run:
    runs-on: ubuntu-latest

    steps:
      # Step 1: Checkout your repository
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 # IMPORTANT: ensures latest commit

      - name: Print current commit
        run: |
          echo "Current commit:"
          git rev-parse HEAD
          echo "Latest commit on main:"
          git rev-parse origin/main

      - name: Print load.py contents
        run: |
          echo "==== LOAD FILE CONTENTS ===="
          sed -n '1,200p' pipeline/load/load.py

      # Step 2: Set up Python environment
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      # Step 3: Install dependencies
      - name: Install dependencies
        run: python -m pip install -r requirements.txt

      # Step 4: Decode Google Cloud credentials from base64
      - name: Decode Google Cloud credentials
        env:
          GOOGLE_SERVICE_ACCOUNT_B64: ${{ secrets.GOOGLE_SERVICE_ACCOUNT_B64 }}
        run: |
          echo "$GOOGLE_SERVICE_ACCOUNT_B64" | base64 --decode > /tmp/gcp-key.json
          chmod 600 /tmp/gcp-key.json
          echo "GOOGLE_JSON_KEY_FILE_PATH=/tmp/gcp-key.json" >> $GITHUB_ENV
          echo "GOOGLE_APPLICATION_CREDENTIALS=/tmp/gcp-key.json" >> $GITHUB_ENV
          echo "Decoded service account JSON written to /tmp/gcp-key.json"

      # Step 5: Run ingestion + load scripts
      - name: Run ingestion and load scripts
        env:
          KROGER_CLIENT_ID: ${{ secrets.KROGER_CLIENT_ID }}
          KROGER_CLIENT_SECRET: ${{ secrets.KROGER_CLIENT_SECRET }}
          BQ_PROJECT: daniel-grocery-project
        run: |
          echo "Running ingestion script for milk, eggs, and oranges..."
          python pipeline/ingestion/latest_file.py milk eggs oranges

          echo "Running load script to upload data to BigQuery..."
          python pipeline/load/load.py

      # Step 6: Set up dbt
      - name: Set up dbt
        run: python -m pip install dbt-bigquery

      # Step 6.5: Configure dbt profile (CI) - BULLETPROOF PATH + VERIFY
      - name: Configure dbt profile
        run: |
          mkdir -p /home/runner/.dbt
          echo "Creating dbt profile..."

          cat > /home/runner/.dbt/profiles.yml <<EOF
grocery_dbt_project:
  target: prod
  outputs:
    prod:
      type: bigquery
      method: service-account
      keyfile: /tmp/gcp-key.json
      project: daniel-grocery-project
      dataset: products_dataset
      threads: 4
      timeout_seconds: 300
      location: US
EOF

          echo "Profiles directory contents:"
          ls -la /home/runner/.dbt
          echo "Profiles file contents:"
          sed -n '1,200p' /home/runner/.dbt/profiles.yml

      # Step 7: Run dbt tests
      - name: Run dbt tests
        run: |
          echo "Running dbt tests..."
          cd pipeline/transformation/dbt_project
          dbt debug --profiles-dir /home/runner/.dbt
          dbt test  --profiles-dir /home/runner/.dbt